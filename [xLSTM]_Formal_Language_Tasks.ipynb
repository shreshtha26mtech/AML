{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo -q"
      ],
      "metadata": {
        "id": "sypdPgTA6MDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AtskjhPDt9rZ",
        "outputId": "3693062a-9419-4e5c-8c8e-b9f8a393a481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/117.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.5/91.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.0/349.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install xlstm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FcJ5W2juCK3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchinfo import summary\n",
        "\n",
        "from xlstm import (\n",
        "    xLSTMBlockStack,\n",
        "    xLSTMBlockStackConfig,\n",
        "    mLSTMBlockConfig,\n",
        "    mLSTMLayerConfig,\n",
        "    sLSTMBlockConfig,\n",
        "    sLSTMLayerConfig,\n",
        "    FeedForwardConfig,\n",
        ")\n",
        "\n",
        "SEQ_LENGTH_XLSTM = 150"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_xlstm_model(seq_length, num_blocks, slstm_pos, num_heads=2, conv1d_kernel_size=2, proj_factor=1.1):\n",
        "    # Define your input size, hidden size, and other relevant parameters\n",
        "    input_size = 1  # Number of features in your time series\n",
        "    embedding_dim = 64  # Dimension of the embeddings, reduced to save memory\n",
        "    output_size = 1  # Number of output features (predicting the next value)\n",
        "\n",
        "    # Define the xLSTM configuration\n",
        "    cfg = xLSTMBlockStackConfig(\n",
        "        mlstm_block=mLSTMBlockConfig(\n",
        "            mlstm=mLSTMLayerConfig(\n",
        "                conv1d_kernel_size=conv1d_kernel_size, qkv_proj_blocksize=2, num_heads=num_heads  # Reduced parameters to save memory\n",
        "            )\n",
        "        ),\n",
        "        slstm_block=sLSTMBlockConfig(\n",
        "            slstm=sLSTMLayerConfig(\n",
        "                backend=\"cuda\",\n",
        "                num_heads=num_heads,  # Reduced number of heads to save memory\n",
        "                conv1d_kernel_size=conv1d_kernel_size,  # Reduced kernel size to save memory\n",
        "                bias_init=\"powerlaw_blockdependent\",\n",
        "            ),\n",
        "            feedforward=FeedForwardConfig(proj_factor=proj_factor, act_fn=\"gelu\"),  # Reduced projection factor to save memory\n",
        "        ),\n",
        "        context_length=seq_length,\n",
        "        num_blocks=num_blocks,  # Reduced number of blocks to save memory\n",
        "        embedding_dim=embedding_dim,\n",
        "        slstm_at=slstm_pos,\n",
        "    )\n",
        "\n",
        "    # Instantiate the xLSTM stack\n",
        "    xlstm_stack = xLSTMBlockStack(cfg).to(\"cuda\")\n",
        "\n",
        "    # Add a linear layer to project input data to the required embedding dimension\n",
        "    input_projection = nn.Linear(input_size, embedding_dim).to(\"cuda\")\n",
        "\n",
        "    # Add a final linear layer to project the xLSTM output to the desired output size\n",
        "    output_projection = nn.Linear(embedding_dim, output_size).to(\"cuda\")\n",
        "\n",
        "    return xlstm_stack, input_projection, output_projection"
      ],
      "metadata": {
        "id": "0gpdSbA52FUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "def timeit(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        architecture = kwargs.get('architecture', 'Unknown Architecture')\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"\\nArchitecture - {architecture} took {elapsed_time:.2f} seconds to train.\")\n",
        "        return result\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "ae2K8vFimYFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "@timeit\n",
        "def train_model(epochs, model, input_projection, output_projection, train_data, optimizer, criterion, architecture=\"Unnamed\"):\n",
        "\n",
        "    losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    start = time.perf_counter()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        progress = tqdm(train_data, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "\n",
        "        for inputs, targets in progress:\n",
        "            projected_input_data = input_projection(inputs)\n",
        "            xlstm_output = model(projected_input_data)\n",
        "            predictions = output_projection(xlstm_output)  # Last timestep output\n",
        "\n",
        "            predictions = predictions.squeeze()\n",
        "            batch_y = targets.squeeze()\n",
        "\n",
        "            loss = criterion(predictions, batch_y)\n",
        "            losses.append(loss.cpu().detach().item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            progress.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_data)\n",
        "        print(f\"Epoch {epoch+1}: Avg Loss = {avg_epoch_loss:.6f}\")\n",
        "        losses.append(avg_epoch_loss)\n",
        "\n",
        "    end = time.perf_counter()\n",
        "    elapsed = end - start\n",
        "    return losses, elapsed\n"
      ],
      "metadata": {
        "id": "yny7TxehB8yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 3\n",
        "SEQUENCE_LENGTH = 8"
      ],
      "metadata": {
        "id": "yPirantYT8_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_val_split(dataset, train_percent=0.7, val_percent=0.15):\n",
        "    total_len = len(dataset)\n",
        "    train_size = int(total_len * train_percent)\n",
        "    val_size = int(total_len * val_percent)\n",
        "    test_size = total_len - train_size - val_size\n",
        "\n",
        "    train_data = dataset[:train_size]\n",
        "    val_data = dataset[train_size:train_size + val_size]\n",
        "    test_data = dataset[train_size + val_size:]\n",
        "\n",
        "    return train_data, val_data, test_data"
      ],
      "metadata": {
        "id": "loBMN373s4SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_dataset(dataset, seq_len=SEQUENCE_LENGTH):\n",
        "    dataX, dataY = [], []\n",
        "\n",
        "    for i in range(len(dataset) - seq_len):\n",
        "        a = dataset[i:(i + seq_len - 1)]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + seq_len - 1])\n",
        "\n",
        "    return torch.Tensor(np.array(dataX)).to('cuda'), torch.Tensor(np.array(dataY)).to('cuda')"
      ],
      "metadata": {
        "id": "WNxKJc0DT09m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def create_dataloader(x, y, is_train=True):\n",
        "    return DataLoader(TensorDataset(x, y), batch_size=BATCH_SIZE, shuffle=True if is_train else False)"
      ],
      "metadata": {
        "id": "45LrJiWZtEn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(losses, title=None):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.title(title)\n",
        "    plt.plot(range(len(losses)), losses)\n",
        "    plt.xlabel('Num. Batch Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5GWwS0xYxVbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH7jpB3cxXx6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ─── Helper Generators for Modular Tasks ───────────────────────────────────\n",
        "\n",
        "def generate_one_expression_and_result(modulus: int, length: int, mult: bool = False):\n",
        "    \"\"\"Generates a modular arithmetic expression with brackets and its result.\"\"\"\n",
        "    def gen_terminal():\n",
        "        terminal = np.random.randint(low=0, high=modulus)\n",
        "        return str(terminal), terminal\n",
        "\n",
        "    if length < 1:\n",
        "        raise ValueError(f'Length must be ≥1, got {length}')\n",
        "    if length == 1:\n",
        "        return gen_terminal()\n",
        "    if length == 2:\n",
        "        s, v = gen_terminal()\n",
        "        return f'-{s}', (-v) % modulus\n",
        "    if length == 3:\n",
        "        s, v = gen_terminal()\n",
        "        return f'({s})', v % modulus\n",
        "    if length == 4:\n",
        "        s, v = gen_terminal()\n",
        "        return f'(-{s})', (-v) % modulus\n",
        "\n",
        "    # otherwise split\n",
        "    left_len  = np.random.randint(1, length-2)\n",
        "    right_len = length - (left_len + 3)\n",
        "    ls, lv = generate_one_expression_and_result(modulus, left_len,  mult)\n",
        "    rs, rv = generate_one_expression_and_result(modulus, right_len, mult)\n",
        "    maxop = 3 if mult else 2\n",
        "    op = np.random.randint(0, maxop)\n",
        "    if op == 0:\n",
        "        return f'({ls}+{rs})', (lv + rv) % modulus\n",
        "    if op == 1:\n",
        "        return f'({ls}-{rs})', (lv - rv) % modulus\n",
        "    return f'({ls}*{rs})', (lv * rv) % modulus\n",
        "\n",
        "def generate_raw_dataset(n: int, lengths: list, modulus: int, mult: bool=False):\n",
        "    \"\"\"\n",
        "    Generates modular‐arithmetic‐with‐brackets data.\n",
        "    Returns dict: length → {'expressions': [...], 'results': [...]}\n",
        "    \"\"\"\n",
        "    alphabet_to_int = {'+':modulus, '-':modulus+1, '*':modulus+2, '(':modulus+3, ')':modulus+4}\n",
        "    for d in range(modulus):\n",
        "        alphabet_to_int[str(d)] = d\n",
        "\n",
        "    out = {}\n",
        "    for L in lengths:\n",
        "        exprs, res = [], []\n",
        "        for _ in range(n // len(lengths)):\n",
        "            s, v = generate_one_expression_and_result(modulus, L, mult)\n",
        "            exprs.append([alphabet_to_int[c] for c in s])\n",
        "            res.append(v)\n",
        "        out[L] = {'expressions': exprs, 'results': res}\n",
        "    return out\n",
        "\n",
        "def generate_equation_and_solution(modulus: int, length: int):\n",
        "    \"\"\"Generates a modular equation with 'x' unknown and its solution.\"\"\"\n",
        "    expr, val = generate_one_expression_and_result(modulus, length-2, mult=False)\n",
        "    # pick a digit to replace with x\n",
        "    idx = np.random.randint(0, len(expr))\n",
        "    digits = [str(d) for d in range(modulus)]\n",
        "    while expr[idx] not in digits:\n",
        "        idx = (idx+1) % len(expr)\n",
        "    sol = int(expr[idx])\n",
        "    eq = f\"{expr[:idx]}x{expr[idx+1:]}={val}\"\n",
        "    return eq, sol\n",
        "\n",
        "def generate_raw_equation_dataset(n: int, lengths: list, modulus: int):\n",
        "    \"\"\"\n",
        "    Generates modular‐equation‐solving data.\n",
        "    Returns dict: length → {'equations': [...], 'solutions': [...]}\n",
        "    \"\"\"\n",
        "    alphabet_to_int = {'+':modulus, '-':modulus+1, '(':modulus+2, ')':modulus+3, 'x':modulus+4, '=':modulus+5}\n",
        "    for d in range(modulus):\n",
        "        alphabet_to_int[str(d)] = d\n",
        "\n",
        "    out = {}\n",
        "    for L in lengths:\n",
        "        eqs, sols = [], []\n",
        "        for _ in range(n // len(lengths)):\n",
        "            s, v = generate_equation_and_solution(modulus, L)\n",
        "            eqs.append([alphabet_to_int[c] for c in s])\n",
        "            sols.append(v)\n",
        "        out[L] = {'equations': eqs, 'solutions': sols}\n",
        "    return out\n",
        "\n",
        "# ─── 1. Bucket Sort ───────────────────────────────────────────────────────\n",
        "\n",
        "def bucket_sort_dataset(n: int, length: int):\n",
        "    \"\"\"\n",
        "    X: (n, length,1), y: (n, length,1) sorted values\n",
        "    \"\"\"\n",
        "    X = np.random.random((n, length, 1))\n",
        "    y = np.sort(X, axis=1)\n",
        "    return X, y\n",
        "\n",
        "# ─── 2. Missing Duplicates ────────────────────────────────────────────────\n",
        "\n",
        "def missing_duplicates_dataset(n: int, length: int):\n",
        "    \"\"\"\n",
        "    X: (n, length,1), y: pad unique values to length\n",
        "    \"\"\"\n",
        "    X = np.random.random((n, length, 1))\n",
        "    y = np.zeros((n, length, 1), dtype=X.dtype)\n",
        "    for i, seq in enumerate(X):\n",
        "        uniq = np.unique(seq.squeeze())\n",
        "        L = uniq.shape[0]\n",
        "        y[i, :L, 0] = uniq\n",
        "    return X, y\n",
        "\n",
        "# ─── 3. Modular Arithmetic with Brackets ──────────────────────────────────\n",
        "\n",
        "def modular_arith_dataset(n: int, lengths: list, modulus: int, mult: bool=False):\n",
        "    raw = generate_raw_dataset(n, lengths, modulus, mult)\n",
        "    out = {}\n",
        "    for L in lengths:\n",
        "        exprs = raw[L]['expressions']\n",
        "        res   = raw[L]['results']\n",
        "        X = np.eye(modulus+5)[exprs]   # one-hot size = digits + +,-,*,(,)\n",
        "        y = np.array(res)\n",
        "        out[L] = (X, y)\n",
        "    return out\n",
        "\n",
        "# ─── 4. Solve Equation for x ──────────────────────────────────────────────\n",
        "\n",
        "def solve_equation_dataset(n: int, lengths: list, modulus: int):\n",
        "    raw = generate_raw_equation_dataset(n, lengths, modulus)\n",
        "    out = {}\n",
        "    for L in lengths:\n",
        "        eqs = raw[L]['equations']\n",
        "        sols= raw[L]['solutions']\n",
        "        X = np.eye(modulus+6)[eqs]     # one-hot size = digits + +,-,(,),x,=\n",
        "        y = np.array(sols)\n",
        "        out[L] = (X, y)\n",
        "    return out\n",
        "\n",
        "# ─── 5. Cycle Navigation ──────────────────────────────────────────────────\n",
        "\n",
        "def cycle_navigation_dataset(n: int, length: int, cycle_length: int=5):\n",
        "    actions = np.random.randint(0,3, size=(n, length))\n",
        "    final   = (np.sum(actions-1, axis=1) % cycle_length)\n",
        "    X = np.eye(3)[actions]\n",
        "    y = np.eye(cycle_length)[final]\n",
        "    return X, y\n",
        "\n",
        "# ─── 6. Even Pairs ────────────────────────────────────────────────────────\n",
        "\n",
        "def even_pairs_dataset(n: int, length: int):\n",
        "    bits   = np.random.randint(0,2, size=(n, length))\n",
        "    unequal= np.logical_xor(bits[:,:-1], bits[:,1:])\n",
        "    labels = np.sum(unequal, axis=1) % 2\n",
        "    X = np.eye(2)[bits]\n",
        "    y = np.eye(2)[labels]\n",
        "    return X, y\n",
        "\n",
        "# ─── 7. Parity Check ──────────────────────────────────────────────────────\n",
        "\n",
        "def parity_check_dataset(n: int, length: int):\n",
        "    bits   = np.random.randint(0,2, size=(n, length))\n",
        "    labels = np.sum(bits, axis=1) % 2\n",
        "    X = np.eye(2)[bits]\n",
        "    y = np.eye(2)[labels]\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Duplicates"
      ],
      "metadata": {
        "id": "9Qkq7qwVyXTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "k_u6JjGyvgRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_SAMPLES = 10000\n",
        "SEQ_LENGTH = 8"
      ],
      "metadata": {
        "id": "Vs2x412D1HTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = missing_duplicates_dataset(N_SAMPLES, SEQ_LENGTH)\n",
        "\n",
        "# 2. Train/test split (80/20)\n",
        "split_idx = int(0.8 * N_SAMPLES)\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# 3. Convert to torch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32).to('cuda')\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).to('cuda')\n",
        "X_test  = torch.tensor(X_test,  dtype=torch.float32).to('cuda')\n",
        "y_test  = torch.tensor(y_test,  dtype=torch.float32).to('cuda')\n",
        "\n",
        "# 4. Create DataLoaders\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "U696Y7iIztjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xlstm_stack, input_proj, output_proj = create_xlstm_model(\n",
        "    seq_length=SEQ_LENGTH,\n",
        "    num_blocks=2,\n",
        "    slstm_pos=[1],\n",
        "    num_heads=2,\n",
        "    conv1d_kernel_size=2,\n",
        "    proj_factor=1.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtamBIToynyF",
        "outputId": "74a3c087-e8c6-40ac-e4cd-3bc63235ae4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xlstm/blocks/slstm/cell.py:543: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @conditional_decorator(\n",
            "/usr/local/lib/python3.11/dist-packages/xlstm/blocks/slstm/cell.py:568: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @conditional_decorator(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(\n",
        "    list(xlstm_stack.parameters()) +\n",
        "    list(input_proj.parameters()) +\n",
        "    list(output_proj.parameters()),\n",
        "    lr=1e-3\n",
        ")\n",
        "\n",
        "# Since this is regression (sorting real values), use MSE\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "nVeyrLlsypEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targ, inp = next(iter(train_loader))\n",
        "targ.shape, inp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOpdNPZ80NL2",
        "outputId": "fa8d5550-168d-4442-bd9f-f483ea8914a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 8, 1]), torch.Size([3, 8, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses, duration = train_model(\n",
        "    epochs=20,\n",
        "    model=xlstm_stack,\n",
        "    input_projection=input_proj,\n",
        "    output_projection=output_proj,\n",
        "    train_data=train_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    architecture=\"xLSTM-MissingDuplicates\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ygrxyB-y56h",
        "outputId": "97c93214-9da5-411c-c221-68a9ee051f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Avg Loss = 0.012612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Avg Loss = 0.011576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Avg Loss = 0.011052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Avg Loss = 0.010697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Avg Loss = 0.010448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Avg Loss = 0.010312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Avg Loss = 0.010259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Avg Loss = 0.010151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Avg Loss = 0.010070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Avg Loss = 0.010027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Avg Loss = 0.010027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Avg Loss = 0.009935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Avg Loss = 0.009946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Avg Loss = 0.009936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Avg Loss = 0.009888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Avg Loss = 0.009878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Avg Loss = 0.009859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Avg Loss = 0.009849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Avg Loss = 0.009800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Avg Loss = 0.009819\n",
            "\n",
            "Architecture - xLSTM-MissingDuplicates took 752.65 seconds to train.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_qk8UFYWzXFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}